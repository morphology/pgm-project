\documentclass{article}

\usepackage{nips12submit_e}
\usepackage{times}
\usepackage{url}

\title{Unsupervised morphology induction}


\author{
Victor Chahuneau\\
\texttt{vchahune@cs.cmu.edu}
\And
Phani Gadde\\
\texttt{pgadde@cs.cmu.edu} \\
\And
Peter Schulam\\
\texttt{pschulam@cs.cmu.edu}
}

\nipsfinalcopy

\begin{document}

\maketitle

\section{Introduction}
Building accurate morphological analyzers is a time-consuming task,
which requires linguistic knowledge and abilities to formalize
morphological phenomena into finite-state rules. This approach has
been successful for several European languages, but the majority of
languages still lack such resources. Unsupervised methods are
therefore an interesting alternative that has been extensively
explored and several approaches -- mostly based on
information-theoretic criteria (MDL) -- have been proposed to solve
this problem. Recently, probabilistic models making use of
non-parametric Bayesian techniques have shown competitive performance.

In particular, Goldwater \& al. \cite{goldwater2011} propose a
baseline model for modeling types and tokens in morphological
induction. Lee \& al. \cite{lee2011} suggest an extension which takes
context into account, while Dreyer \& Eisner \cite{dreyer2011} add
structure by encoding morphological phenomena into paradigms that are
tied to grammatical functions, but evaluate their model in a
semi-supervised setting.

\section{Scope}
\label{sec:scope}

Broadly, our goal is to develop new models of morphology that merge
the attractive ideas that have been presented in the papers listed
above. For example, the idea of using syntactic context to help model
morphological inflection is attractive from a linguistic standpoint,
but we believe that we can improve upon the modeling decisions made by
\cite{lee2011}. In particular, \cite{lee2011} use geometric
distributions over the length of a morpheme to model the probability
of a language's lexicon. We do not agree with this modeling decision
since the geometric distribution does not take the morpheme itself
into account. We hope to implement a more linguistically sound model
of the lexicon, but still incorporate models of syntactic context.

Also, \cite{dreyer2011} attempt to model morphological
paradigms, which is again linguistically attractive. An issue,
however, is that distributions over string-valued random variables
within morphological paradigms are modeled using weighted finite state
transducers, and learning these distributions requires complex
variants of message passing that manipulate transducers to update the
model. This is complex to implement, and it is not clear that this
parameterization outperforms alternatives. We would like to use the
idea of morphological paradigms in our model, but make simpler
parameterization decisions.

\section{Datasets}
Training unsupervised models only requires monolingual corpora, which
are easily available for a large variety of morphologically rich
languages. Evaluation, however, has to be done on analyzed data, and
we plan to use the following standard datasets: English, Finnish,
German and Turkish corpora from the Morpho
Challenge\footnote{\url{http://research.ics.aalto.fi/events/morphochallenge2010/datasets.shtml}},
and the Arabic\footnote{\url{http://www.ircs.upenn.edu/arabic/}},
Czech\footnote{\url{http://ufal.mff.cuni.cz/pcedt2.0/}} and
Korean\footnote{\url{http://www.cis.upenn.edu/~xtag/koreantag/}}
treebanks, which contain morphological annotations. Additionaly, we
plan to induce latent syntactic categories for words as part of our
model, and we can evaluate this on POS tagged data.

\section{Implementation}
We will write software to train models and evaluate them on various
types of datasets. We plan to develop general-purpose modules for
describing probability distributions and sampling methods that should
allow us to try several model configurations based on these elementary
building blocks. Development will be done mostly using a shared public
git repository: \url{https://github.com/morphology/pgm-project}

\section{Planning}
By the midterm, we should have implemented and evaluated a baseline
system similar to \cite{goldwater2011}. We should also have started
exploring ways to improve this baseline by adding contextual
information, structured paradigms, agreement constraints, etc.  Tasks
that can be split between the members of the team are:
\begin{itemize}
  \item Implement and train the baseline system
  \item Compare choices of distributions in the lexicon model
  \item Prepare evaluation scripts for the various datasets in several languages
  \item Analyze errors made by the baseline on a development corpus
  \item Explore block sampling techniques to speed up the inference
\end{itemize}

\bibliographystyle{plain}
\bibliography{bibliography}

\end{document}

\documentclass{article}

\usepackage{amsmath,amssymb}
\usepackage{enumerate}
\usepackage{nips12submit_e}
\usepackage{times}
\usepackage{url}

\title{Faster Unsupervised Morphology Induction}

\author{
Victor Chahuneau\\
\texttt{vchahune@cs.cmu.edu}
\And
Phani Gadde\\
\texttt{pgadde@cs.cmu.edu} \\
\And
Peter Schulam\\
\texttt{pschulam@cs.cmu.edu}
}

\begin{document}

\maketitle

\section{Introduction}
\label{sec:introduction}

Building accurate morphological analyzers is a time-consuming task,
which requires linguistic knowledge and abilities to formalize
morphological phenomena into finite-state rules. This approach has
been successful for several European languages, but the majority of
languages still lack such resources. Unsupervised methods are
therefore an interesting alternative that has been extensively
explored and several approaches -- mostly based on
information-theoretic criteria (MDL) -- have been proposed to solve
this problem. Recently, probabilistic models making use of
non-parametric Bayesian techniques have shown competitive performance.

In particular, Goldwater \& al. \cite{goldwater2011} propose a
baseline model for modeling types and tokens in morphological
induction. Lee \& al. \cite{lee2011} suggest an extension which takes
context into account, while Dreyer \& Eisner \cite{dreyer2011} add
structure by encoding morphological phenomena into paradigms that are
tied to grammatical functions, but evaluate their model in a
semi-supervised setting.

Unsupervised morphological analysis is typically done using complex
nonparametric Bayesian models. Deriving the posterior distribution
over quantities of interest (such as the stem lexicon of a language)
can yield complex mathematical expressions which are difficult to
compute. In some applications of Bayesian statistical modeling, it is
acceptable to run samplers for a long time in order to compute a
useful result, but in natural language processing, and, more
specifically, morphological analysis, the amount of data that must be
processed is often large. Furthermore, as morphological analysis is
typically a step used to preprocess tokens in a document before
applying other types of linguistic analysis, it is not a process on
which we would like to spend too much time.

Crude morphological analysis tools such as the Porter Stemmer are easy
to implement and computationally inexpensive, and thus have been
successful in areas such as information retrieval where we need to
quickly ``analyze'' billions of tokens. Recent Bayesian morphological
models are more linguistically sophisticated, and could potentially
provide more accurate stemming information. Real world application of
these models is not realistic, however, due to the long running time
required when using MCMC methods for inference. Our project aims to
integrate recent advances in speeding up MCMC computation into one of
the fundamental Bayesian morphological models. Our goals are to:

\begin{enumerate}[1.]
\item Reparameterize the model proposed in \cite{goldwater2011} so
  that inference can be distributed.
\item Implement the reparameterization.
\item Evaluate timing and performance differences between original
  model and our reparameterization.
\end{enumerate}

\section{Related Work}
\label{sec:related-work}

\subsection{Parallel Markov Chain Monte Carlo for Dirichlet Process Mixtures}
\label{sec:parallel-mcmc-for-dpm}

\cite{lovell2012} propose a reparameterization of Dirichlet Process
mixture models that allows MCMC inference schemes to be split across
multiple machines. The high-level strategy is to form
``superclusters'' of latent variable labels so that latent variables
for each data point $x_i$ within a supercluster are conditionally
independent of other data points given the supercluster
assignments. This allows the most expensive piece of the computation
(sampling the posterior distribution over latent labels) to be
performed in parallel within each supercluster. We review their
contribution as we derive our reparameterization of
\cite{goldwater2011} from it.

\cite{lovell2012} define the following generative process, which, as
we will show, defines the same target distribution as an unmodified
Dirichlet process with concentration parameter $\alpha$ and base
distribution $H$. We begin by drawing a sample $\boldsymbol{\mu}$ from
a distribution over the $K$ dimensional simplex, where $K$ is the
number of superclusters that we would like in our model. Functionally,
this defines the number of compute nodes over which we would like to
distribute our computation. We now draw another vector
$\boldsymbol{\gamma}$ from the $K$ dimensional simplex, but now draw
from the Dirichlet distribution with parameters $\alpha \mu_1, \ldots,
\alpha \mu_K$. We know draw $K$ random distributions from $K$
independent Dirichlet processes with the base measure $H$. Each of the
Dirichlet processes has concentration parameter $\alpha \mu_i$ where
$i \in \{1, \ldots, K\}$. Finally, we form a distribution $G$ by
mixing $G_1, \ldots, G_k$ with weights $\gamma_1, \ldots,
\gamma_K$. The full generative process is as follows:

\begin{align*}
  \mu_1, \ldots, \mu_K &\sim Dirichlet(\tau) \\
  \gamma_1, \ldots, \gamma_K &\sim Dirichlet(\alpha \mu_1, \ldots, \alpha \mu_K) \\
  G_i &\sim DP(\alpha \mu_i, H) \text{ for } i \in \{1, \ldots, K\} \\
  G &= \sum_{i=1}^K \gamma_i G_i
\end{align*}

They authors of \cite{lovell2012} note that the procedure defined
above results in $G \sim DP(\alpha, H)$. The key property of this
reparameterization of the Dirichlet process is that the standard
Chinese Restaurant Process gets reformulated as a two-stage process in
which a new data point first chooses which restaurant it will eat at,
and then chooses a table based on where other customers of that
restaurant are seated.

We now go through the mathematics of the process in more detail. We
follow the original paper, using $z_n$ to denote the table chosen by
the $n$th data point where $n$ ranges from $\{1, \ldots, N\}$. We let
$j \in \mathbb{N}$ uniquely index the tables across \textit{all}
restaurants, and $s_j = i$ be the supercluster or restaurant in which
the $j$th table is located. We can then express the probability that a
customer chooses restaurant $i$ given $\alpha$ as

\begin{align}
  Pr(s_{z_n} = i | \{z_{n^\prime}\}_{n^{\prime}=1}^{n-1}, \alpha)
  &= \frac{\alpha \mu_i + \sum_{n^\prime = 1}^{n-1} \mathbb{I}(s_{z_{n^\prime}} = i)}{\alpha + n - 1}
\end{align}

This is simply the predictive probability of restaurant $i$ under the
Dirichlet multinomial with prior $Dir(\alpha\boldsymbol{\mu})$. Once a
customer has chosen restaurant $i$, then she chooses a table within
the restaurant according to a local Chinese Restaurant Process

\begin{align}
  P(z_n = j, 1 \le j \le \#tables(i) | \{z_{n^\prime}\}_{n^{\prime}=1}^{n-1}, \alpha, s_j = i)
  &= \frac{\sum_{n^\prime = 1}^{n-1} \mathbb{I}(s_{z_{n^\prime}} = i, z_{n^\prime} = j)}
          {\alpha \mu_i + \sum_{n^\prime = 1}^{n-1} \mathbb{I}(s_{z_{n^\prime}} = i)} \\
  P(z_n = j, j = \#tables(i) + 1 | \{z_{n^\prime}\}_{n^{\prime}=1}^{n-1}, \alpha, s_j = i)
  &= \frac{\alpha \mu_i}{\alpha \mu_i + \sum_{n^\prime = 1}^{n-1} \mathbb{I}(s_{z_{n^\prime}} = i)}
\end{align}

The key property of the expressions above that allow us to parallelize
the MCMC inference algorithm is that the probability of a customer
sitting at a table $j$ conditioned on the restaurant $i$ is
independent of all customers that are not seated at that restaurant
(since the sums in both the numerator and denominator in the first
expression and the denominator in the second expression do not count
$s_{z_{n^\prime}} \neq i$). Therefore, when resampling seating
arrangements, which is often the most computationally intensive aspect
of MCMC in Dirichlet process mixture models, we can reseat customers
in each restaurant on different compute nodes.

\subsection{Transition Operators}
\label{sec:parallel-transition-operators}

Following \cite{lovell2012}, we define the following relevant counts

\begin{align*}
  \#i = \sum_{n=1}^N \mathbb{I}(s_{z_n} = i),\text{ }
  \#j = \sum_{n=1}^N \mathbb{I}(z_n = j),\text{ }
  J_i = \sum_{j=1}^\infty \mathbb{I}(\#k > 0, s_j = i)
\end{align*}

We now discuss the distributions from which each variable in the model
described above can be sampled. This includes $\alpha$ (the
concentration parameter of the primary Dirichlet process), $z_n$ (the
seating arrangements of the customers), and $s_j$ (the restaurant
assignment for each cluster $j$). Note that we do not discuss the
model parameters $\theta_j$ for each cluster $j$. \cite{lovell2012} do
not devote much attention to this and briefly mention that this is
model specific, but can sometimes be done in parallel. For our model
of morphology, however, this is not the case. Indeed, the models of
morphology that we use require global knowledge of the current
analysis for types in the corpus. We therefore put discussion of the
model-specific parameters off until the next section.

The first parameter we can resample is the concentration
$\alpha$. This should be resampled conditioned on the seating
arrangements $\{z_n\}_{n=1}^N$. Assuming a prior $\pi(\alpha)$, the
posterior over $\alpha$ is

\begin{align}
  p(\alpha | \{z_n\}_{n=1}^N) \propto p(\alpha) p(\{z_n\}_{n=1}^N | \alpha)
\end{align}

\cite{lovell2012} show that the likelihood of the seating arrangements
and the cluster assignments to restaurants can be expressed as

\begin{align}
  P(\{z_n\}_{n=1}^N, \{s_j\}_{j=1}^\infty | \alpha) &=
  \frac{\Gamma(\alpha)}{\Gamma(N + \alpha)} \alpha^{\sum_{i=1}^K J_i \prod_{i=1}^K \mu_i^{J_i}}
\end{align}

which is simply the marginal Chinese Restaurant Process multipled by
an additional factor that accounts for how the clusters are assigned
to each restaurant (i.e. the supercluster assignments). This allows us
to then rewrite the posterior over $\alpha$ as

\begin{align}
  p(\alpha | \{z_n\}_{n=1}^N) \propto p(\alpha) \frac{\Gamma(\alpha)}{\Gamma(N + \alpha)} \alpha^{\sum_{i=1}^K J_i \prod_{i=1}^K \mu_i^{J_i}
\end{align}

\section{Proposed Method}
\label{sec:proposed-method}

\section{Experiments}
\label{sec:experiments}

\section{Conclusion}
\label{sec:conclusion}

\bibliographystyle{plain}
\bibliography{bibliography}

\end{document}
